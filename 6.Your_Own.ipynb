{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from google.colab import drive\n",
    "drive.mount('/content/drive')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append('/content/drive/MyDrive/Colab Notebooks')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "from matplotlib import pyplot as plt\n",
    "\n",
    "import imp\n",
    "try:\n",
    "    imp.find_module('jupyterplot')\n",
    "    from jupyterplot import ProgressPlot\n",
    "except ImportError:\n",
    "    !pip install jupyterplot\n",
    "    from jupyterplot import ProgressPlot\n",
    "\n",
    "import torch\n",
    "from torch import nn\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "import torchvision\n",
    "from torchvision import datasets as D\n",
    "from torchvision import transforms as T\n",
    "\n",
    "from utils import train_step, test_step\n",
    "from utils import get_cifar10_dataset, make_dataloader\n",
    "from utils import simulate_scheduler"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 지금까지 배운 내용을 조합해 자신의 모델 훈련해보기"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model\n",
    "- Convolution layer 개수: input / output feature의 크기가 같도록 하면 늘릴 수 있음\n",
    "- Convolution layer의 out_channels 조절해보기\n",
    "- Fully connected layer(nn.Linear)의 features 조절해보기 / 개수 조절해보기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MyCNN(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        ##### layers here ####\n",
    "    \n",
    "    def forward(self, x):\n",
    "        #### convolutions ####\n",
    "        \n",
    "        ######################\n",
    "        x = x.flatten(1)\n",
    "        ####      FCs     ####\n",
    "        \n",
    "        ######################\n",
    "        return x\n",
    "\n",
    "print(MyCNN())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 예시\n",
    "예시를 확인하고 자신의 네트워크를 만든 후에는 아래 셀을 지워주세요"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MyCNN(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        ##### layers here ####\n",
    "        self.conv1 = nn.Conv2d(\n",
    "            in_channels=3, out_channels=32,\n",
    "            kernel_size=5, stride=1, padding=2\n",
    "        )\n",
    "        self.relu1 = nn.ReLU(inplace=True)\n",
    "        self.conv1_1 = nn.Conv2d(32, 32, 3, 2, 1)\n",
    "        self.relu1_1 = nn.ReLU(inplace=True)\n",
    "\n",
    "        self.conv2 = nn.Conv2d(32, 64, 3, 1, 1)\n",
    "        self.relu2 = nn.ReLU(inplace=True)\n",
    "        self.conv2_1 = nn.Conv2d(64, 64, 3, 2, 1)\n",
    "        self.relu2_1 = nn.ReLU(inplace=True)\n",
    "\n",
    "        self.conv3 = nn.Conv2d(64, 128, 3, 1, 1)\n",
    "        self.relu3 = nn.ReLU(inplace=True)\n",
    "        self.conv3_1 = nn.Conv2d(128, 128, 3, 2, 1)\n",
    "        self.relu3_1 = nn.ReLU(inplace=True)\n",
    "        \n",
    "        self.fc1 = nn.Linear(in_features=128 * 4 * 4, out_features=64)\n",
    "        self.fc2 = nn.Linear(64, 10)\n",
    "        \n",
    "        # 파라미터 초기화 예시\n",
    "        for module in self.modules():\n",
    "            if isinstance(module, nn.Conv2d):\n",
    "                torch.nn.init.kaiming_normal_(module.weight, nonlinearity='relu')\n",
    "                # torch.nn.init.zeros_(module.bias)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        #### convolutions ####\n",
    "        x = self.relu1(self.conv1(x))\n",
    "        x = self.relu1_1(self.conv1_1(x))\n",
    "\n",
    "        x = self.relu2(self.conv2(x))\n",
    "        x = self.relu2_1(self.conv2_1(x))\n",
    "\n",
    "        x = self.relu3(self.conv3(x))\n",
    "        x = self.relu3_1(self.conv3_1(x))\n",
    "        \n",
    "        ######################\n",
    "        x = x.flatten(1)\n",
    "        ####      FCs     ####\n",
    "        x = self.fc1(x)\n",
    "        x = self.fc2(x)\n",
    "        ######################\n",
    "        return x\n",
    "\n",
    "print(MyCNN())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Hyperparameters\n",
    "\n",
    "- Epoch 수\n",
    "- learning rate\n",
    "- (optional) batch_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "momentum = 0.9\n",
    "phases = ['train', 'test']\n",
    "\n",
    "num_epochs = 1\n",
    "learning_rate = 0.01\n",
    "batch_size = 64"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Augmentation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_augmentation = False\n",
    "\n",
    "dataset = get_cifar10_dataset(random_crop=data_augmentation)\n",
    "loader = make_dataloader(dataset, batch_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Learning rate scheduling\n",
    "- gamma: 한 epoch당 얼만큼 줄일 것인가. 숫자가 낮을수록 빠르게 줄어듦"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gamma = 0.9\n",
    "lrs = simulate_scheduler(gamma, num_epochs)\n",
    "plt.plot(lrs)\n",
    "plt.xlabel('epoch')\n",
    "plt.ylabel('learning rate')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = MyCNN().to(device)\n",
    "optimizer = torch.optim.SGD(model.parameters(), learning_rate, 0.9)\n",
    "# optimizer = torch.optim.Adam(model.parameters(), 0.01) # Adam optimizer 사용입니다. learning rate는 0.01 사용 권장.\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "scheduler = torch.optim.lr_scheduler.ExponentialLR(optimizer, gamma)\n",
    "\n",
    "pp = ProgressPlot(\n",
    "    plot_names=phases,\n",
    "    line_names=['loss', 'accuracy'],\n",
    "    x_lim=[0, None],\n",
    "    x_label='Iteration',\n",
    "    y_lim=[[0, None], [0, 100]]\n",
    ")\n",
    "\n",
    "accuracy = 0\n",
    "for epoch in range(num_epochs):\n",
    "    for inputs, target in loader['train']:\n",
    "        loss = train_step(model, inputs, target, optimizer, criterion, device)\n",
    "        pp.update([[loss, -1], [-500, accuracy]])\n",
    "    \n",
    "    corrects = 0\n",
    "    for inputs, target in loader['test']:\n",
    "        output, _ = test_step(model, inputs, target, device=device)\n",
    "        corrects += (output.argmax(1).cpu() == target).sum().item()\n",
    "    accuracy = corrects / len(dataset['test']) * 100\n",
    "    \n",
    "    print(f'Epoch: {epoch+1} accuracy {accuracy:.2f}')\n",
    "pp.finalize()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "",
   "name": ""
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
