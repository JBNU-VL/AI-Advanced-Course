{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append('path/of/AI-Advanced-Course')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "from copy import deepcopy\n",
    "from matplotlib import pyplot as plt\n",
    "\n",
    "import imp\n",
    "try:\n",
    "    imp.find_module('jupyterplot')\n",
    "    from jupyterplot import ProgressPlot\n",
    "except ImportError:\n",
    "    !pip install jupyterplot\n",
    "    from jupyterplot import ProgressPlot\n",
    "\n",
    "import torch\n",
    "from torch import nn\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "import torchvision\n",
    "from torchvision import datasets as D\n",
    "from torchvision import transforms as T\n",
    "\n",
    "from utils import train_step, test_step\n",
    "from utils import simulate_scheduler\n",
    "from utils import BaselineModel\n",
    "from utils import get_cifar10_dataset, make_dataloader\n",
    "from utils import fetch_data, sample_random_data, show_images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "num_epochs = 50\n",
    "batch_size = 64\n",
    "learning_rate = 0.01\n",
    "momentum = 0.9\n",
    "phases = ['train', 'test']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "normal_dataset = get_cifar10_dataset()\n",
    "random_augment_dataset = get_cifar10_dataset(random_crop=True)\n",
    "loader = make_dataloader(normal_dataset, batch_size)\n",
    "rloader = make_dataloader(random_augment_dataset, batch_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 일반 이미지"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "images, target = fetch_data(normal_dataset['train'], [10, 100, 1000, 10000, 52])\n",
    "titles = [normal_dataset['train'].classes[idx] for idx in target]\n",
    "show_images(images.permute(0,2,3,1), titles)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 무작위 확대 / 이동 한 이미지"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "images, target = fetch_data(random_augment_dataset['train'], [10, 100, 1000, 10000, 52])\n",
    "titles = [normal_dataset['train'].classes[idx] for idx in target]\n",
    "show_images(images.permute(0,2,3,1), titles)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "base_cnn = BaselineModel()\n",
    "nets = [\n",
    "    BaselineModel().to(device)\n",
    "    for _ in range(2)\n",
    "]\n",
    "_ = [net.load_state_dict(deepcopy(base_cnn.state_dict())) for net in nets]\n",
    "\n",
    "optimizers = [\n",
    "    torch.optim.SGD(net.parameters(), learning_rate, 0.9)\n",
    "    for net in nets\n",
    "]\n",
    "scheds = [\n",
    "    torch.optim.lr_scheduler.ExponentialLR(optimizer, 0.9)\n",
    "    for optimizer in optimizers\n",
    "]\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "conditions = ['without_augmentation', 'with_augmentation']\n",
    "pp = ProgressPlot(\n",
    "    plot_names=phases,\n",
    "    line_names=conditions,\n",
    "    x_lim=[0, num_epochs*len(loader['train'])],\n",
    "    x_label='Iteration',\n",
    "    y_lim=[[0, 3], [50, 100]]\n",
    ")\n",
    "\n",
    "accs = [0, 0]\n",
    "for epoch in range(num_epochs):\n",
    "    for nbatch, rbatch in zip(loader['train'], rloader['train']):\n",
    "        losses = [\n",
    "            train_step(net, *batch, optimizer, criterion, device)\n",
    "            for net, optimizer, batch in zip(nets, optimizers, [nbatch, rbatch])\n",
    "        ]\n",
    "        pp.update([losses, accs])\n",
    "    \n",
    "    corrects = [0, 0]\n",
    "    for inputs, target in loader['test']:\n",
    "        outputs = [\n",
    "            test_step(net, inputs, target, device=device)[0]\n",
    "            for net in nets\n",
    "        ]\n",
    "        corrects = [\n",
    "            (correct + (output.argmax(1).cpu() == target).sum()).item()\n",
    "            for correct, output in zip(corrects, outputs)\n",
    "        ]\n",
    "        \n",
    "    accs = [\n",
    "        correct / len(normal_dataset['test']) * 100\n",
    "        for correct in corrects\n",
    "    ]\n",
    "    \n",
    "    print(f'Epoch: {epoch+1} accuracy ', end='')\n",
    "    for cond, acc in zip(conditions, accs):\n",
    "        print(f'{cond}: {acc:.2f}', end=' ')\n",
    "    print()\n",
    "    [sched.step() for sched in scheds]\n",
    "pp.finalize()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "kd-attention",
   "language": "python",
   "name": "kd-attention"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
